services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    volumes:
      - zk-data:/var/lib/zookeeper/data
      - zk-txn-logs:/var/lib/zookeeper/log
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTRA:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTRA://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: INTRA://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INTRA
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD","bash","-lc","kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 5s
      timeout: 5s
      retries: 30
    volumes:
      - kafka_data:/var/lib/kafka/data
  
  # NEW: Kafka UI (Provectus)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    ports:
      - "8081:8080"

  airflow-db:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    logging:
      driver: "local"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - airflow-db:/var/lib/postgresql/data

  # ---- One-off init (migrations + admin user) ----
  airflow-init:
    build:
      context: ./airflow
    env_file: ../config/.env
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    depends_on:
      airflow-db:
        condition: service_healthy
    entrypoint: /bin/bash
    command:
      - -c
      - |
          set -euo pipefail
          echo "Waiting for Postgres..."
          until pg_isready -h airflow-db -p 5432 >/dev/null 2>&1; do sleep 1; done
          echo "Running DB migrations..."
          airflow db upgrade || airflow db init
          echo "Creating admin user (idempotent)..."
          airflow users create \
            --username "${_AIRFLOW_WWW_USER_USERNAME:-admin}" \
            --password "${_AIRFLOW_WWW_USER_PASSWORD:-admin}" \
            --firstname "${_AIRFLOW_WWW_USER_FIRSTNAME:-Admin}" \
            --lastname "${_AIRFLOW_WWW_USER_LASTNAME:-User}" \
            --role Admin \
            --email "${_AIRFLOW_WWW_USER_EMAIL:-admin@example.com}" || true
          echo "Init complete."
    restart: "no"


  # ---- Webserver ----
  airflow-webserver:
    build:
      context: ./airflow
    env_file: ../config/.env
    environment:
      AIRFLOW__WEBSERVER__WORKERS: "1"
      AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT: "300"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      # --- added for AWS/Kafka ---
      AWS_REGION: "eu-central-1"
      CLICKSTREAM_S3_BUCKET: "clickstream-522814689373-dev-euc1"
      KAFKA_BOOTSTRAP: "kafka:9092"
      KAFKA_TOPIC: "clickstream.events"
      # ensure containers can reach EC2 metadata (IMDS) even if a proxy is set
      NO_PROXY: "169.254.169.254,localhost,127.0.0.1"
      no_proxy: "169.254.169.254,localhost,127.0.0.1"
    logging:
      driver: "local"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    volumes:
      - ../airflow/dags:/opt/airflow/dags:ro
      - ../airflow/plugins:/opt/airflow/plugins:ro
      - ../airflow/include:/opt/airflow/include:ro
    command: airflow webserver

  # ---- Scheduler ----
  airflow-scheduler:
    build:
      context: ./airflow
    env_file: ../config/.env
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__SCHEDULER__PARSING_PROCESSES: "1"
      # --- added for AWS/Kafka ---
      AWS_REGION: "eu-central-1"
      CLICKSTREAM_S3_BUCKET: "clickstream-522814689373-dev-euc1"
      KAFKA_BOOTSTRAP: "kafka:9092"
      KAFKA_TOPIC: "clickstream.events"
      NO_PROXY: "169.254.169.254,localhost,127.0.0.1"
      no_proxy: "169.254.169.254,localhost,127.0.0.1"
    logging:
      driver: "local"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
      - ../airflow/dags:/opt/airflow/dags:ro
      - ../airflow/plugins:/opt/airflow/plugins:ro
      - ../airflow/include:/opt/airflow/include:ro
    command: airflow scheduler


  kafka-producer:
    build:
      context: ../kafka/producer
    env_file: ../config/.env
    depends_on:
      kafka:
        condition: service_healthy
    command: python produce_events.py

volumes:
  zk-data:
  zk-txn-logs:
  airflow-db:
  kafka_data:
